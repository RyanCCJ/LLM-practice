{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Building RAG Agents\n",
        "The following will demonstrate how to use **LangChain** to build your own RAG agent, including chunking documents, constructing vector stores, and implementing the RAG chain."
      ],
      "metadata": {
        "id": "5t4RXSqqi1yD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import and Setup\n",
        "Remember to fill in your **Gemini API Key** in this block"
      ],
      "metadata": {
        "id": "UDZXlmzzjQ00"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f48sWDK3eF-Q"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain-community\n",
        "!pip install langchain-google-genai\n",
        "!pip install faiss-cpu\n",
        "!pip install pymupdf arxiv\n",
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "API_KEY=\"<Your-API-Key>\""
      ],
      "metadata": {
        "id": "k4JVrha-jYgX"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LangChain Basic"
      ],
      "metadata": {
        "id": "hL4GcHi8S795"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "model = GoogleGenerativeAI(model=\"gemini-1.5-flash\", google_api_key=API_KEY)\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Only respond in rhymes\"),\n",
        "    (\"user\", \"{input}\")\n",
        "])\n",
        "\n",
        "rhyme_chain = prompt | model | StrOutputParser()\n",
        "\n",
        "print(rhyme_chain.invoke({\"input\" : \"Tell me about yourself!\"}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37Hbn5VJlAFx",
        "outputId": "a662b3ca-e577-4eb9-82f8-3a7b530ff8e7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A language model, here I stand,\n",
            "With words at hand, I'll do my best to understand.\n",
            "To learn and grow, my purpose is to serve,\n",
            "To answer questions, and to make you observe. \n",
            "I'm a tool for knowledge, a friend to you,\n",
            "With information vast, I'll see you through. \n",
            "So tell me your desires, your thoughts, your fears,\n",
            "And I'll respond with rhymes, through all the years. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embeddings and Vector Stores"
      ],
      "metadata": {
        "id": "Fj_-if5aTCh2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", google_api_key=API_KEY)\n",
        "embedder = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=API_KEY)\n",
        "\n",
        "vector = embedder.embed_query(\"Hello World!\")\n",
        "vector[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SS78EUJgREb",
        "outputId": "62a07436-4340-4941-cabf-144f5cf9a2ed"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.05169878527522087,\n",
              " -0.033477481454610825,\n",
              " -0.031893402338027954,\n",
              " -0.029319265857338905,\n",
              " 0.019925475120544434]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "from pprint import pprint\n",
        "\n",
        "conversation = [\n",
        "    \"[User]  Hello! My name is Beras, and I'm a big blue bear! Can you please tell me about the rocky mountains?\",\n",
        "    \"[Agent] The Rocky Mountains are a beautiful and majestic range of mountains that stretch across North America\",\n",
        "    \"[Beras] Wow, that sounds amazing! Ive never been to the Rocky Mountains before, but Ive heard many great things about them.\",\n",
        "    \"[Agent] I hope you get to visit them someday, Beras! It would be a great adventure for you!\"\n",
        "    \"[Beras] Thank you for the suggestion! Ill definitely keep it in mind for the future.\",\n",
        "    \"[Agent] In the meantime, you can learn more about the Rocky Mountains by doing some research online or watching documentaries about them.\"\n",
        "    \"[Beras] I live in the arctic, so I'm not used to the warm climate there. I was just curious, ya know!\",\n",
        "    \"[Agent] Absolutely! Lets continue the conversation and explore more about the Rocky Mountains and their significance!\"\n",
        "]\n",
        "\n",
        "vector_store = FAISS.from_texts(conversation, embedding=embedder)\n",
        "retriever = vector_store.as_retriever()\n",
        "\n",
        "pprint(retriever.invoke(\"What is your name?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bv0hvwiHPdhk",
        "outputId": "fabb1349-ce47-4620-f545-824c60136cce"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(metadata={}, page_content=\"[User]  Hello! My name is Beras, and I'm a big blue bear! Can you please tell me about the rocky mountains?\"),\n",
            " Document(metadata={}, page_content='[Agent] Absolutely! Lets continue the conversation and explore more about the Rocky Mountains and their significance!'),\n",
            " Document(metadata={}, page_content='[Agent] The Rocky Mountains are a beautiful and majestic range of mountains that stretch across North America'),\n",
            " Document(metadata={}, page_content='[Beras] Wow, that sounds amazing! Ive never been to the Rocky Mountains before, but Ive heard many great things about them.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Answer the question using only the context\"\n",
        "    \"\\n\\nRetrieved Context: {context}\"\n",
        "    \"\\n\\nUser Question: {question}\"\n",
        "    \"\\nAnswer the user conversationally. User is not aware of context.\"\n",
        ")\n",
        "\n",
        "chain = (\n",
        "    {'context': vector_store.as_retriever(),\n",
        "     'question': (lambda x: x)}\n",
        "    | context_prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "pprint(chain.invoke(\"Where does Beras live?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TifqLDVnQnAs",
        "outputId": "3ef968bd-61c0-40d3-d3e1-83ebed13fb3e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Based on the conversation, Beras lives in the arctic! \\n'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(chain.invoke(\"Where are the Rocky Mountains?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ds1Zfkb_RyfU",
        "outputId": "daec31d0-b788-4010-dfa2-257609ec1173"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(\"The Rocky Mountains are a mountain range in North America! They're known for \"\n",
            " 'being beautiful and majestic. \\n')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(chain.invoke(\"How far away is Beras from the Rocky Mountains?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAVJzy3xR2-P",
        "outputId": "64a50872-3981-451e-901f-2eb00b29ba65"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(\"We don't have enough information to know how far Beras is from the Rocky \"\n",
            " \"Mountains. We know Beras lives in the arctic, but we don't know exactly \"\n",
            " 'where that is.  \\n')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build a RAG Agent\n",
        "We download several papers about AI and use our custom RAG agent to answer user's questions."
      ],
      "metadata": {
        "id": "yOsilXtVXfFN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading And Chunking Documents"
      ],
      "metadata": {
        "id": "IjYmGCeuke_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import ArxivLoader\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000, chunk_overlap=100,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \";\", \",\", \" \"],\n",
        ")\n",
        "\n",
        "print(\"Loading Documents\")\n",
        "docs = [\n",
        "    ArxivLoader(query=\"1706.03762\").load(),  ## Attention Is All You Need Paper\n",
        "    ArxivLoader(query=\"1810.04805\").load(),  ## BERT Paper\n",
        "    ArxivLoader(query=\"2005.11401\").load(),  ## RAG Paper\n",
        "    ArxivLoader(query=\"2205.00445\").load(),  ## MRKL Paper\n",
        "    ArxivLoader(query=\"2310.06825\").load(),  ## Mistral Paper\n",
        "    ArxivLoader(query=\"2306.05685\").load(),  ## LLM-as-a-Judge\n",
        "]\n",
        "\n",
        "## Cut the paper short if references is included.\n",
        "for doc in docs:\n",
        "    content = json.dumps(doc[0].page_content)\n",
        "    if \"References\" in content:\n",
        "        doc[0].page_content = content[:content.index(\"References\")]\n",
        "\n",
        "## Split the documents and also filter out stubs (overly short chunks)\n",
        "print(\"Chunking Documents\\n\")\n",
        "docs_chunks = [ text_splitter.split_documents(doc) for doc in docs ]\n",
        "docs_chunks = [[c for c in dchunks if len(c.page_content) > 200] for dchunks in docs_chunks]\n",
        "\n",
        "## Make some custom Chunks to give big-picture details\n",
        "doc_string = \"Available Documents:\"\n",
        "doc_metadata = []\n",
        "for chunks in docs_chunks:\n",
        "    metadata = getattr(chunks[0], 'metadata', {})\n",
        "    doc_string += \"\\n - \" + metadata.get('Title')\n",
        "    doc_metadata += [ str(metadata) ]\n",
        "\n",
        "extra_chunks = [ doc_string ] + doc_metadata\n",
        "\n",
        "## Printing out some summary information for reference\n",
        "pprint(doc_string)\n",
        "print()\n",
        "for i, chunks in enumerate(docs_chunks):\n",
        "    print(f\"Document {i}\")\n",
        "    print(f\" - # Chunks: {len(chunks)}\")\n",
        "    print(f\" - Metadata: \")\n",
        "    pprint(chunks[0].metadata)\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqWWWeUlSFbV",
        "outputId": "d2d6e660-1883-4ff1-f6aa-e86e346b8305"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Documents\n",
            "Chunking Documents\n",
            "\n",
            "('Available Documents:\\n'\n",
            " ' - Attention Is All You Need\\n'\n",
            " ' - BERT: Pre-training of Deep Bidirectional Transformers for Language '\n",
            " 'Understanding\\n'\n",
            " ' - Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\\n'\n",
            " ' - MRKL Systems: A modular, neuro-symbolic architecture that combines large '\n",
            " 'language models, external knowledge sources and discrete reasoning\\n'\n",
            " ' - Mistral 7B\\n'\n",
            " ' - Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena')\n",
            "\n",
            "Document 0\n",
            " - # Chunks: 35\n",
            " - Metadata: \n",
            "{'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion '\n",
            "            'Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
            " 'Published': '2023-08-02',\n",
            " 'Summary': 'The dominant sequence transduction models are based on complex '\n",
            "            'recurrent or\\n'\n",
            "            'convolutional neural networks in an encoder-decoder '\n",
            "            'configuration. The best\\n'\n",
            "            'performing models also connect the encoder and decoder through an '\n",
            "            'attention\\n'\n",
            "            'mechanism. We propose a new simple network architecture, the '\n",
            "            'Transformer, based\\n'\n",
            "            'solely on attention mechanisms, dispensing with recurrence and '\n",
            "            'convolutions\\n'\n",
            "            'entirely. Experiments on two machine translation tasks show these '\n",
            "            'models to be\\n'\n",
            "            'superior in quality while being more parallelizable and requiring '\n",
            "            'significantly\\n'\n",
            "            'less time to train. Our model achieves 28.4 BLEU on the WMT 2014\\n'\n",
            "            'English-to-German translation task, improving over the existing '\n",
            "            'best results,\\n'\n",
            "            'including ensembles by over 2 BLEU. On the WMT 2014 '\n",
            "            'English-to-French\\n'\n",
            "            'translation task, our model establishes a new single-model '\n",
            "            'state-of-the-art\\n'\n",
            "            'BLEU score of 41.8 after training for 3.5 days on eight GPUs, a '\n",
            "            'small fraction\\n'\n",
            "            'of the training costs of the best models from the literature. We '\n",
            "            'show that the\\n'\n",
            "            'Transformer generalizes well to other tasks by applying it '\n",
            "            'successfully to\\n'\n",
            "            'English constituency parsing both with large and limited training '\n",
            "            'data.',\n",
            " 'Title': 'Attention Is All You Need'}\n",
            "\n",
            "Document 1\n",
            " - # Chunks: 45\n",
            " - Metadata: \n",
            "{'Authors': 'Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova',\n",
            " 'Published': '2019-05-24',\n",
            " 'Summary': 'We introduce a new language representation model called BERT, '\n",
            "            'which stands\\n'\n",
            "            'for Bidirectional Encoder Representations from Transformers. '\n",
            "            'Unlike recent\\n'\n",
            "            'language representation models, BERT is designed to pre-train '\n",
            "            'deep\\n'\n",
            "            'bidirectional representations from unlabeled text by jointly '\n",
            "            'conditioning on\\n'\n",
            "            'both left and right context in all layers. As a result, the '\n",
            "            'pre-trained BERT\\n'\n",
            "            'model can be fine-tuned with just one additional output layer to '\n",
            "            'create\\n'\n",
            "            'state-of-the-art models for a wide range of tasks, such as '\n",
            "            'question answering\\n'\n",
            "            'and language inference, without substantial task-specific '\n",
            "            'architecture\\n'\n",
            "            'modifications.\\n'\n",
            "            '  BERT is conceptually simple and empirically powerful. It '\n",
            "            'obtains new\\n'\n",
            "            'state-of-the-art results on eleven natural language processing '\n",
            "            'tasks, including\\n'\n",
            "            'pushing the GLUE score to 80.5% (7.7% point absolute '\n",
            "            'improvement), MultiNLI\\n'\n",
            "            'accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 '\n",
            "            'question answering\\n'\n",
            "            'Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 '\n",
            "            'Test F1 to 83.1\\n'\n",
            "            '(5.1 point absolute improvement).',\n",
            " 'Title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language '\n",
            "          'Understanding'}\n",
            "\n",
            "Document 2\n",
            " - # Chunks: 46\n",
            " - Metadata: \n",
            "{'Authors': 'Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, '\n",
            "            'Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, '\n",
            "            'Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela',\n",
            " 'Published': '2021-04-12',\n",
            " 'Summary': 'Large pre-trained language models have been shown to store '\n",
            "            'factual knowledge\\n'\n",
            "            'in their parameters, and achieve state-of-the-art results when '\n",
            "            'fine-tuned on\\n'\n",
            "            'downstream NLP tasks. However, their ability to access and '\n",
            "            'precisely manipulate\\n'\n",
            "            'knowledge is still limited, and hence on knowledge-intensive '\n",
            "            'tasks, their\\n'\n",
            "            'performance lags behind task-specific architectures. '\n",
            "            'Additionally, providing\\n'\n",
            "            'provenance for their decisions and updating their world knowledge '\n",
            "            'remain open\\n'\n",
            "            'research problems. Pre-trained models with a differentiable '\n",
            "            'access mechanism to\\n'\n",
            "            'explicit non-parametric memory can overcome this issue, but have '\n",
            "            'so far been\\n'\n",
            "            'only investigated for extractive downstream tasks. We explore a '\n",
            "            'general-purpose\\n'\n",
            "            'fine-tuning recipe for retrieval-augmented generation (RAG) -- '\n",
            "            'models which\\n'\n",
            "            'combine pre-trained parametric and non-parametric memory for '\n",
            "            'language\\n'\n",
            "            'generation. We introduce RAG models where the parametric memory '\n",
            "            'is a\\n'\n",
            "            'pre-trained seq2seq model and the non-parametric memory is a '\n",
            "            'dense vector index\\n'\n",
            "            'of Wikipedia, accessed with a pre-trained neural retriever. We '\n",
            "            'compare two RAG\\n'\n",
            "            'formulations, one which conditions on the same retrieved passages '\n",
            "            'across the\\n'\n",
            "            'whole generated sequence, the other can use different passages '\n",
            "            'per token. We\\n'\n",
            "            'fine-tune and evaluate our models on a wide range of '\n",
            "            'knowledge-intensive NLP\\n'\n",
            "            'tasks and set the state-of-the-art on three open domain QA tasks, '\n",
            "            'outperforming\\n'\n",
            "            'parametric seq2seq models and task-specific retrieve-and-extract '\n",
            "            'architectures.\\n'\n",
            "            'For language generation tasks, we find that RAG models generate '\n",
            "            'more specific,\\n'\n",
            "            'diverse and factual language than a state-of-the-art '\n",
            "            'parametric-only seq2seq\\n'\n",
            "            'baseline.',\n",
            " 'Title': 'Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks'}\n",
            "\n",
            "Document 3\n",
            " - # Chunks: 40\n",
            " - Metadata: \n",
            "{'Authors': 'Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher '\n",
            "            'Lieber, Nir Ratner, Yoav Shoham, Hofit Bata, Yoav Levine, Kevin '\n",
            "            'Leyton-Brown, Dor Muhlgay, Noam Rozen, Erez Schwartz, Gal '\n",
            "            'Shachaf, Shai Shalev-Shwartz, Amnon Shashua, Moshe Tenenholtz',\n",
            " 'Published': '2022-05-01',\n",
            " 'Summary': 'Huge language models (LMs) have ushered in a new era for AI, '\n",
            "            'serving as a\\n'\n",
            "            'gateway to natural-language-based knowledge tasks. Although an '\n",
            "            'essential\\n'\n",
            "            'element of modern AI, LMs are also inherently limited in a number '\n",
            "            'of ways. We\\n'\n",
            "            'discuss these limitations and how they can be avoided by adopting '\n",
            "            'a systems\\n'\n",
            "            'approach. Conceptualizing the challenge as one that involves '\n",
            "            'knowledge and\\n'\n",
            "            'reasoning in addition to linguistic processing, we define a '\n",
            "            'flexible\\n'\n",
            "            'architecture with multiple neural models, complemented by '\n",
            "            'discrete knowledge\\n'\n",
            "            'and reasoning modules. We describe this neuro-symbolic '\n",
            "            'architecture, dubbed the\\n'\n",
            "            'Modular Reasoning, Knowledge and Language (MRKL, pronounced '\n",
            "            '\"miracle\") system,\\n'\n",
            "            'some of the technical challenges in implementing it, and '\n",
            "            \"Jurassic-X, AI21 Labs'\\n\"\n",
            "            'MRKL system implementation.',\n",
            " 'Title': 'MRKL Systems: A modular, neuro-symbolic architecture that combines '\n",
            "          'large language models, external knowledge sources and discrete '\n",
            "          'reasoning'}\n",
            "\n",
            "Document 4\n",
            " - # Chunks: 21\n",
            " - Metadata: \n",
            "{'Authors': 'Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris '\n",
            "            'Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian '\n",
            "            'Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, '\n",
            "            'Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le '\n",
            "            'Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El '\n",
            "            'Sayed',\n",
            " 'Published': '2023-10-10',\n",
            " 'Summary': 'We introduce Mistral 7B v0.1, a 7-billion-parameter language '\n",
            "            'model engineered\\n'\n",
            "            'for superior performance and efficiency. Mistral 7B outperforms '\n",
            "            'Llama 2 13B\\n'\n",
            "            'across all evaluated benchmarks, and Llama 1 34B in reasoning, '\n",
            "            'mathematics, and\\n'\n",
            "            'code generation. Our model leverages grouped-query attention '\n",
            "            '(GQA) for faster\\n'\n",
            "            'inference, coupled with sliding window attention (SWA) to '\n",
            "            'effectively handle\\n'\n",
            "            'sequences of arbitrary length with a reduced inference cost. We '\n",
            "            'also provide a\\n'\n",
            "            'model fine-tuned to follow instructions, Mistral 7B -- Instruct, '\n",
            "            'that surpasses\\n'\n",
            "            'the Llama 2 13B -- Chat model both on human and automated '\n",
            "            'benchmarks. Our\\n'\n",
            "            'models are released under the Apache 2.0 license.',\n",
            " 'Title': 'Mistral 7B'}\n",
            "\n",
            "Document 5\n",
            " - # Chunks: 44\n",
            " - Metadata: \n",
            "{'Authors': 'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, '\n",
            "            'Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric '\n",
            "            'P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica',\n",
            " 'Published': '2023-12-24',\n",
            " 'Summary': 'Evaluating large language model (LLM) based chat assistants is '\n",
            "            'challenging\\n'\n",
            "            'due to their broad capabilities and the inadequacy of existing '\n",
            "            'benchmarks in\\n'\n",
            "            'measuring human preferences. To address this, we explore using '\n",
            "            'strong LLMs as\\n'\n",
            "            'judges to evaluate these models on more open-ended questions. We '\n",
            "            'examine the\\n'\n",
            "            'usage and limitations of LLM-as-a-judge, including position, '\n",
            "            'verbosity, and\\n'\n",
            "            'self-enhancement biases, as well as limited reasoning ability, '\n",
            "            'and propose\\n'\n",
            "            'solutions to mitigate some of them. We then verify the agreement '\n",
            "            'between LLM\\n'\n",
            "            'judges and human preferences by introducing two benchmarks: '\n",
            "            'MT-bench, a\\n'\n",
            "            'multi-turn question set; and Chatbot Arena, a crowdsourced battle '\n",
            "            'platform. Our\\n'\n",
            "            'results reveal that strong LLM judges like GPT-4 can match both '\n",
            "            'controlled and\\n'\n",
            "            'crowdsourced human preferences well, achieving over 80% '\n",
            "            'agreement, the same\\n'\n",
            "            'level of agreement between humans. Hence, LLM-as-a-judge is a '\n",
            "            'scalable and\\n'\n",
            "            'explainable way to approximate human preferences, which are '\n",
            "            'otherwise very\\n'\n",
            "            'expensive to obtain. Additionally, we show our benchmark and '\n",
            "            'traditional\\n'\n",
            "            'benchmarks complement each other by evaluating several variants '\n",
            "            'of LLaMA and\\n'\n",
            "            'Vicuna. The MT-bench questions, 3K expert votes, and 30K '\n",
            "            'conversations with\\n'\n",
            "            'human preferences are publicly available at\\n'\n",
            "            'https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.',\n",
            " 'Title': 'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena'}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Construct Document Vector Stores"
      ],
      "metadata": {
        "id": "vsh28r_3ka9W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Constructing Vector Stores\")\n",
        "vecstores = [FAISS.from_texts(extra_chunks, embedder)]\n",
        "vecstores += [FAISS.from_documents(doc_chunks, embedder) for doc_chunks in docs_chunks]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTqxUu0YYvyv",
        "outputId": "e3d16bac-f73c-456c-f105-42b3b510a662"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Constructing Vector Stores\n",
            "CPU times: user 251 ms, sys: 10.8 ms, total: 262 ms\n",
            "Wall time: 3.67 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from faiss import IndexFlatL2\n",
        "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
        "\n",
        "embed_dims = len(embedder.embed_query(\"test\"))\n",
        "def default_FAISS():\n",
        "    '''Useful utility for making an empty FAISS vectorstore'''\n",
        "    return FAISS(\n",
        "        embedding_function=embedder,\n",
        "        index=IndexFlatL2(embed_dims),\n",
        "        docstore=InMemoryDocstore(),\n",
        "        index_to_docstore_id={},\n",
        "        normalize_L2=False\n",
        "    )\n",
        "\n",
        "def aggregate_vstores(vectorstores):\n",
        "    ## Initialize an empty FAISS Index and merge others into it\n",
        "    ## We'll use default_faiss for simplicity, though it's tied to your embedder by reference\n",
        "    agg_vstore = default_FAISS()\n",
        "    for vstore in vectorstores:\n",
        "        agg_vstore.merge_from(vstore)\n",
        "    return agg_vstore\n",
        "\n",
        "## Unintuitive optimization; merge_from seems to optimize constituent vector stores away\n",
        "docstore = aggregate_vstores(vecstores)\n",
        "\n",
        "print(f\"Constructed aggregate docstore with {len(docstore.docstore._dict)} chunks\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlP2a0NZcG0Q",
        "outputId": "cd2e20d8-3f52-4460-ee29-74db26e52783"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Constructed aggregate docstore with 238 chunks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implement RAG Chain"
      ],
      "metadata": {
        "id": "4f0bM2q2kpxC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables.passthrough import RunnableAssign\n",
        "\n",
        "convstore = default_FAISS()\n",
        "\n",
        "def save_memory_and_get_output(d, vstore):\n",
        "    \"\"\"Accepts 'input'/'output' dictionary and saves to convstore\"\"\"\n",
        "    vstore.add_texts([\n",
        "        f\"User previously responded with {d.get('input')}\",\n",
        "        f\"Agent previously responded with {d.get('output')}\"\n",
        "    ])\n",
        "    return d.get('output')\n",
        "\n",
        "initial_msg = (\n",
        "    \"Hello! I am a document chat agent here to help the user!\"\n",
        "    f\" I have access to the following documents: {doc_string}\\n\\nHow can I help you?\"\n",
        ")\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([(\"system\",\n",
        "    \"You are a document chatbot. Help the user as they ask questions about documents.\"\n",
        "    \" User messaged just asked: {input}\\n\\n\"\n",
        "    \" From this, we have retrieved the following potentially-useful info: \"\n",
        "    \" Conversation History Retrieval:\\n{history}\\n\\n\"\n",
        "    \" Document Retrieval:\\n{context}\\n\\n\"\n",
        "    \" (Answer only from retrieval. Only cite sources that are used. Make your response conversational.)\"\n",
        "), ('user', '{input}')])\n",
        "\n",
        "stream_chain = chat_prompt | llm | StrOutputParser()\n",
        "retrieval_chain = (\n",
        "    {'input' : (lambda x: x)}\n",
        "    | RunnableAssign({'history' : (lambda x: x['input']) | convstore.as_retriever()})\n",
        "    | RunnableAssign({'context' : (lambda x: x['input']) | docstore.as_retriever()})\n",
        ")\n",
        "\n",
        "\n",
        "def chat_gen(message, history=[], return_buffer=True):\n",
        "    buffer = \"\"\n",
        "    retrieval = retrieval_chain.invoke(message)\n",
        "    line_buffer = \"\"\n",
        "\n",
        "    for token in stream_chain.stream(retrieval):\n",
        "        buffer += token\n",
        "        yield buffer if return_buffer else token\n",
        "\n",
        "    save_memory_and_get_output({'input':  message, 'output': buffer}, convstore)\n",
        "\n",
        "\n",
        "## Start of Agent Event Loop\n",
        "test_question = \"Tell me about RAG!\"  ## <- modify as desired\n",
        "\n",
        "for response in chat_gen(test_question, return_buffer=False):\n",
        "    print(response, end='')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jxPzCiwdomn",
        "outputId": "2b0f0b1e-8205-4d94-9453-b055c354a59d"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAG stands for Retrieval-Augmented Generation. It's a pretty cool technique used in natural language processing (NLP) that combines the strengths of two different types of models:  parametric and non-parametric. \n",
            "\n",
            "Think of it like this: The parametric model is like a really smart person who has learned a lot from reading lots of books, but might not know everything. The non-parametric model is like a massive library, full of information but needing someone to find the right books. \n",
            "\n",
            "RAG brings these two together. It uses a pre-trained model (like a seq2seq model) as its \"smart person\" and a dense vector index of information (like Wikipedia) as its \"library\".  A neural retriever is used to find the most relevant information from the library, and this information is then used to help the model generate more accurate and factual responses. \n",
            "\n",
            "In short, RAG helps NLP models be more knowledge-intensive, making them better at tasks like answering open-domain questions and generating more factual language. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interact With Gradio Chatbot"
      ],
      "metadata": {
        "id": "hm9g9ezEk0Xw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "chatbot = gr.Chatbot(value = [[None, initial_msg]])\n",
        "demo = gr.ChatInterface(chat_gen, chatbot=chatbot).queue()\n",
        "\n",
        "try:\n",
        "  demo.launch(debug=True, share=True, show_api=False)\n",
        "  demo.close()\n",
        "except Exception as e:\n",
        "  demo.close()\n",
        "  print(e)\n",
        "  raise e"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 694
        },
        "id": "8DZwvtsQfTE0",
        "outputId": "9276fcac-ecb3-4906-d9cf-e06ac2cbe627"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gradio/components/chatbot.py:223: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://53951c0fefc1d7f3ed.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://53951c0fefc1d7f3ed.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7862 <> https://53951c0fefc1d7f3ed.gradio.live\n",
            "Closing server running on port: 7862\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save and Reload"
      ],
      "metadata": {
        "id": "uwNHCSr2k3N3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docstore.save_local(\"docstore_index\")\n",
        "!tar czvf docstore_index.tgz docstore_index\n",
        "!rm -rf docstore_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2x8xlZzxg2eB",
        "outputId": "13c98834-aad4-4a72-debb-2800869ce349"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "docstore_index/\n",
            "docstore_index/index.faiss\n",
            "docstore_index/index.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar xzvf docstore_index.tgz\n",
        "new_db = FAISS.load_local(\"docstore_index\", embedder, allow_dangerous_deserialization=True)\n",
        "docs = new_db.similarity_search(\"Tell me about Mistral 7B\")\n",
        "print(docs[0].page_content[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6PD29ZWhmfG",
        "outputId": "7160ff7a-050f-4db4-e759-3b3178e451cc"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "docstore_index/\n",
            "docstore_index/index.faiss\n",
            "docstore_index/index.pkl\n",
            ". We also provide a model fine-tuned to follow instructions,\\nMistral 7B \\u2013 Instruct, that surpasses Llama 2 13B \\u2013 chat model both on human and\\nautomated benchmarks. Our models are released under the Apache 2.0 license.\\nCode: https://github.com/mistralai/mistral-src\\nWebpage: https://mistral.ai/news/announcing-mistral-7b/\\n1\\nIntroduction\\nIn the rapidly evolving domain of Natural Language Processing (NLP), the race towards higher model\\nperformance often necessitates an escalation in model size. However, this scaling tends to increase\\ncomputational costs and inference latency, thereby raising barriers to deployment in practical,\\nreal-world scenarios. In this context, the search for balanced models delivering both high-level\\nperformance and efficiency becomes critically essential. Our model, Mistral 7B, demonstrates that\\na carefully designed language model can deliver high performance while maintaining an efficient\\ninference\n"
          ]
        }
      ]
    }
  ]
}