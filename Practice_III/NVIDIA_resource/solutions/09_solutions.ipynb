{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8274060e-1926-4641-9ab9-a948a58db514",
   "metadata": {},
   "source": [
    "## LangServe Server Setup\n",
    "\n",
    "This notebook is a playground for those interested in developing interactive web applications using LangChain and [**LangServe**](https://python.langchain.com/docs/langserve). The aim is to provide a minimal-code example to illustrate the potential of LangChain in web application contexts.\n",
    "\n",
    "This section provides a walkthrough for setting up a simple API server using LangChain's Runnable interfaces with FastAPI. The example demonstrates how to integrate a LangChain model, such as `ChatNVIDIA`, to create and distribute accessible API routes. Using this, you will be able to supply functionality to the frontend service's [**`frontend_server.py`**](frontend/frontend_server.py) session, which strongly expects:\n",
    "- A simple endpoint named `:9012/basic_chat` for the basic chatbot, exemplified below.\n",
    "- A pair of endpoints named `:9012/retriever` and `:9012/generator` for the RAG chatbot.\n",
    "- All three for the **Evaluate** utility, which will be required for the final assessment. *More on that later!*\n",
    "\n",
    "**IMPORTANT NOTES:**\n",
    "- Make sure to click the square ( $\\square$ ) button twice to shut down an active FastAPI cell. The first time might fall through or trigger a try-catch routine on an asynchronous process.\n",
    "- If it still doesn't work, do a hard restart on this notebook by using **Kernel -> Restart Kernel**.\n",
    "- When a FastAPI server is running in your cell, expect the process to block up this notebook. Other notebooks should not be impacted by this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41d21a2-6d72-4c5e-b523-647b4d52b1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile server_app.py\n",
    "# https://python.langchain.com/docs/langserve#server\n",
    "from fastapi import FastAPI\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "from langserve import add_routes\n",
    "\n",
    "## May be useful later\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.prompt_values import ChatPromptValue\n",
    "from langchain_core.runnables import RunnableLambda, RunnableBranch, RunnablePassthrough\n",
    "from langchain_core.runnables.passthrough import RunnableAssign\n",
    "from langchain_community.document_transformers import LongContextReorder\n",
    "from functools import partial\n",
    "from operator import itemgetter\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "## TODO: Make sure to pick your LLM and do your prompt engineering as necessary for the final assessment\n",
    "embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\")\n",
    "instruct_llm = ChatNVIDIA(model=\"mistralai/mixtral-8x22b-instruct-v0.1\")\n",
    "llm = instruct_llm | StrOutputParser()\n",
    "\n",
    "## Prompt\n",
    "chat_prompt = ChatPromptTemplate.from_messages([(\"system\",\n",
    "    \"You are a document chatbot. Help the user as they ask questions about documents.\"\n",
    "    \" User messaged just asked you a question: {input}\\n\\n\"\n",
    "    \" The following information may be useful for your response: \"\n",
    "    \" Document Retrieval:\\n{context}\\n\\n\"\n",
    "    \" (Answer only from retrieval. Only cite sources that are used. Make your response conversational)\"\n",
    "), ('user', '{input}')])\n",
    "\n",
    "docstore = FAISS.load_local(\"docstore_index\", embedder, allow_dangerous_deserialization=True)\n",
    "docs = list(docstore.docstore._dict.values())\n",
    "\n",
    "app = FastAPI(\n",
    "  title=\"LangChain Server\",\n",
    "  version=\"1.0\",\n",
    "  description=\"A simple api server using Langchain's Runnable interfaces\",\n",
    ")\n",
    "\n",
    "## PRE-ASSESSMENT: Run as-is and see the basic chain in action\n",
    "\n",
    "add_routes(\n",
    "    app,\n",
    "    instruct_llm,\n",
    "    path=\"/basic_chat\",\n",
    ")\n",
    "\n",
    "## ASSESSMENT TODO: Implement these components as appropriate\n",
    "\n",
    "add_routes(\n",
    "    app,\n",
    "    docstore.as_retriever(),\n",
    "    path=\"/retriever\",\n",
    ")\n",
    "\n",
    "add_routes(\n",
    "    app,\n",
    "    chat_prompt | llm ,\n",
    "    path=\"/generator\",\n",
    ")\n",
    "\n",
    "## Might be encountered if this were for a standalone python file...\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=9012)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
